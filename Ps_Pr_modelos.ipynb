{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tT7wkV2O1KZL",
    "outputId": "d71f3007-5a7b-49e4-b094-c612c71993c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recommenders\n",
      "  Downloading recommenders-1.1.1-py3-none-any.whl (339 kB)\n",
      "\u001b[K     |████████████████████████████████| 339 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk<4,>=3.4\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (1.21.4)\n",
      "Collecting transformers<5,>=2.5.0\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml<6,>=5.4.1\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-macosx_10_9_x86_64.whl (253 kB)\n",
      "\u001b[K     |████████████████████████████████| 253 kB 47.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: seaborn<1,>=0.8.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (0.11.2)\n",
      "Collecting scikit-surprise>=1.0.6\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "\u001b[K     |████████████████████████████████| 771 kB 22.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cornac<2,>=1.1.2\n",
      "  Downloading cornac-1.14.2-cp38-cp38-macosx_10_14_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 116 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.31.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (4.64.0)\n",
      "Requirement already satisfied: scipy<2,>=1.0.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (1.7.3)\n",
      "Collecting pandera[strategies]>=0.6.5\n",
      "  Downloading pandera-0.13.4-py3-none-any.whl (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 31.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lightfm<2,>=1.15\n",
      "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
      "\u001b[K     |████████████████████████████████| 310 kB 27.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (3.0.3)\n",
      "Requirement already satisfied: matplotlib<4,>=2.2.2 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (3.5.1)\n",
      "Collecting numba<1,>=0.38.1\n",
      "  Downloading numba-0.56.4-cp38-cp38-macosx_10_14_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 58.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lightgbm>=2.2.1\n",
      "  Downloading lightgbm-3.3.3-py3-none-macosx_10_15_x86_64.macosx_11_6_x86_64.macosx_12_0_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting category-encoders<2,>=1.3.0\n",
      "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (1.0.2)\n",
      "Requirement already satisfied: pandas<2,>1.0.3 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (1.4.3)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from recommenders) (2.28.1)\n",
      "Collecting bottleneck<2,>=1.2.1\n",
      "  Downloading Bottleneck-1.3.5-cp38-cp38-macosx_10_9_x86_64.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting memory-profiler<1,>=0.54.0\n",
      "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
      "Collecting statsmodels>=0.6.1\n",
      "  Downloading statsmodels-0.13.5-cp38-cp38-macosx_10_9_x86_64.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 70.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting patsy>=0.4.1\n",
      "  Downloading patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 22.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting powerlaw\n",
      "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
      "Requirement already satisfied: wheel in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from lightgbm>=2.2.1->recommenders) (0.37.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from matplotlib<4,>=2.2.2->recommenders) (4.28.5)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.9.4-cp36-abi3-macosx_10_9_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 26.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.10.31-cp38-cp38-macosx_10_9_x86_64.whl (294 kB)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-macosx_10_9_x86_64.whl (25.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.5 MB 70.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from numba<1,>=0.38.1->recommenders) (58.0.4)\n",
      "Requirement already satisfied: importlib-metadata in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from numba<1,>=0.38.1->recommenders) (4.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 27.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-inspect>=0.6.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting hypothesis>=5.41.1\n",
      "  Downloading hypothesis-6.60.0-py3-none-any.whl (398 kB)\n",
      "\u001b[K     |████████████████████████████████| 398 kB 57.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.2.0)\n",
      "Collecting sortedcontainers<3.0.0,>=2.1.0\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting exceptiongroup>=1.0.0\n",
      "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3,>=2.0.0->recommenders) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3,>=2.0.0->recommenders) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3,>=2.0.0->recommenders) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3,>=2.0.0->recommenders) (3.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.0.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 62.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.8.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers<5,>=2.5.0->recommenders) (4.0.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/zeba/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from importlib-metadata->numba<1,>=0.38.1->recommenders) (3.6.0)\n",
      "Collecting mpmath\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[K     |████████████████████████████████| 532 kB 36.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Building wheels for collected packages: lightfm, scikit-surprise\n",
      "  Building wheel for lightfm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightfm: filename=lightfm-1.16-cp38-cp38-macosx_10_9_x86_64.whl size=446310 sha256=03910d0f07dbba4f5ec2e50083a1fd24014c13d4f0e5fe2fbc061df629215332\n",
      "  Stored in directory: /Users/zeba/Library/Caches/pip/wheels/ec/bb/51/9c487d021c1373b691d13cadca0b65b6852627b1f3f43550fa\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp38-cp38-macosx_10_9_x86_64.whl size=1180300 sha256=850ff5ee74a293a874f797ab14e0d26b62f70e76b9b25080fe9015540ce4f53d\n",
      "  Stored in directory: /Users/zeba/Library/Caches/pip/wheels/af/db/86/2c18183a80ba05da35bf0fb7417aac5cddbd93bcb1b92fd3ea\n",
      "Successfully built lightfm scikit-surprise\n",
      "Installing collected packages: typing-extensions, mypy-extensions, wrapt, typing-inspect, sortedcontainers, pyyaml, pydantic, patsy, mpmath, filelock, exceptiongroup, tokenizers, statsmodels, regex, psutil, powerlaw, pandera, llvmlite, hypothesis, huggingface-hub, click, transformers, scikit-surprise, retrying, numba, nltk, memory-profiler, lightgbm, lightfm, cornac, category-encoders, bottleneck, recommenders\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.0\n",
      "    Uninstalling typing-extensions-4.0.0:\n",
      "      Successfully uninstalled typing-extensions-4.0.0\n",
      "Successfully installed bottleneck-1.3.5 category-encoders-1.3.0 click-8.1.3 cornac-1.14.2 exceptiongroup-1.0.4 filelock-3.8.2 huggingface-hub-0.11.1 hypothesis-6.60.0 lightfm-1.16 lightgbm-3.3.3 llvmlite-0.39.1 memory-profiler-0.61.0 mpmath-1.2.1 mypy-extensions-0.4.3 nltk-3.7 numba-0.56.4 pandera-0.13.4 patsy-0.5.3 powerlaw-1.5 psutil-5.9.4 pydantic-1.10.2 pyyaml-5.4.1 recommenders-1.1.1 regex-2022.10.31 retrying-1.3.4 scikit-surprise-1.1.3 sortedcontainers-2.4.0 statsmodels-0.13.5 tokenizers-0.13.2 transformers-4.25.1 typing-extensions-4.4.0 typing-inspect-0.8.0 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LqVXO52J3kKe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
    "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eDw4wtRv8v5z"
   },
   "outputs": [],
   "source": [
    "MIND_VERSION = 'demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bqUuFTLa6Oom"
   },
   "outputs": [],
   "source": [
    "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCbeZO6c7Gzx",
    "outputId": "a9c56d70-949f-4742-8457-b991c50ca24d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 17.0k/17.0k [00:10<00:00, 1.67kKB/s]\n",
      "100%|█████████████████████████████████████| 9.84k/9.84k [00:03<00:00, 2.65kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "\n",
    "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
    "\n",
    "if not os.path.exists(train_news_file):\n",
    "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
    "    \n",
    "if not os.path.exists(valid_news_file):\n",
    "    download_deeprec_resources(mind_url, \\\n",
    "                               os.path.join(data_path, 'valid'), mind_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M-vfEiWo9yvP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "behaviors_col_names = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "\n",
    "train_data = pd.read_table(train_behaviors_file, header=None, names=behaviors_col_names)\n",
    "valid_data = pd.read_table(valid_behaviors_file, header=None, names=behaviors_col_names)\n",
    "\n",
    "def data_to_recommended_and_selected(dataframe: pd.DataFrame, user_col: str = 'UserID', interactions_col: str = 'Impressions'):\n",
    "    data = list()\n",
    "    items = set()\n",
    "    for user, user_interactions in zip(dataframe[user_col].values, dataframe[interactions_col].values):\n",
    "        \n",
    "        # Interações são da forma noticia_id-clicado, exemplo: N23699-0 N21291-0 N1901-1 N27292-0 N17443-0\n",
    "\n",
    "        R = [interaction.split('-')[0] for interaction in user_interactions.split()] \n",
    "        S = [interaction.split('-')[0] for interaction in user_interactions.split() if interaction[-1] == '1']\n",
    "        \n",
    "        items.update(R)\n",
    "\n",
    "        data.append((user, ' '.join(R), ' '.join(S)))\n",
    "\n",
    "    return pd.DataFrame(data, columns=['UserID', 'R', 'S']), items\n",
    "\n",
    "train_data, train_items = data_to_recommended_and_selected(train_data)\n",
    "valid_data, valid_items = data_to_recommended_and_selected(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "t09cL0DkCdsP",
    "outputId": "e2d04188-7325-4d80-a7c7-58b36a25da58"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U82271</td>\n",
       "      <td>N13390 N7180 N20785 N6937 N15776 N25810 N20820...</td>\n",
       "      <td>N15368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U84185</td>\n",
       "      <td>N13089 N18101 N1248 N26273 N12770 N1132 N13649</td>\n",
       "      <td>N12770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U11552</td>\n",
       "      <td>N18390 N10537 N23967</td>\n",
       "      <td>N23967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U68381</td>\n",
       "      <td>N15660 N18609 N2831 N5677 N19010 N1502 N19215 ...</td>\n",
       "      <td>N18390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U52303</td>\n",
       "      <td>N15645 N7911</td>\n",
       "      <td>N7911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22029</th>\n",
       "      <td>U69508</td>\n",
       "      <td>N16297 N15645 N20630 N10602 N27294 N24649 N129...</td>\n",
       "      <td>N9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22030</th>\n",
       "      <td>U79085</td>\n",
       "      <td>N27355 N9809 N20882 N8787 N25926 N3864 N15163 ...</td>\n",
       "      <td>N13282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22031</th>\n",
       "      <td>U46989</td>\n",
       "      <td>N13316 N23592 N10240 N1994 N26998 N17157 N1324...</td>\n",
       "      <td>N13247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22032</th>\n",
       "      <td>U49050</td>\n",
       "      <td>N23982 N17424 N17157 N4324 N22216 N14332 N6221...</td>\n",
       "      <td>N13557 N7050 N18892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22033</th>\n",
       "      <td>U6774</td>\n",
       "      <td>N5109 N7452 N13064 N26428 N22785 N23921 N27197...</td>\n",
       "      <td>N4326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22034 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserID                                                  R  \\\n",
       "0      U82271  N13390 N7180 N20785 N6937 N15776 N25810 N20820...   \n",
       "1      U84185     N13089 N18101 N1248 N26273 N12770 N1132 N13649   \n",
       "2      U11552                               N18390 N10537 N23967   \n",
       "3      U68381  N15660 N18609 N2831 N5677 N19010 N1502 N19215 ...   \n",
       "4      U52303                                       N15645 N7911   \n",
       "...       ...                                                ...   \n",
       "22029  U69508  N16297 N15645 N20630 N10602 N27294 N24649 N129...   \n",
       "22030  U79085  N27355 N9809 N20882 N8787 N25926 N3864 N15163 ...   \n",
       "22031  U46989  N13316 N23592 N10240 N1994 N26998 N17157 N1324...   \n",
       "22032  U49050  N23982 N17424 N17157 N4324 N22216 N14332 N6221...   \n",
       "22033   U6774  N5109 N7452 N13064 N26428 N22785 N23921 N27197...   \n",
       "\n",
       "                         S  \n",
       "0                   N15368  \n",
       "1                   N12770  \n",
       "2                   N23967  \n",
       "3                   N18390  \n",
       "4                    N7911  \n",
       "...                    ...  \n",
       "22029                N9916  \n",
       "22030               N13282  \n",
       "22031               N13247  \n",
       "22032  N13557 N7050 N18892  \n",
       "22033                N4326  \n",
       "\n",
       "[22034 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIND Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../MIND-small'\n",
    "\n",
    "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
    "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
    "\n",
    "valid_news_file = os.path.join(data_path, 'dev', r'news.tsv')\n",
    "valid_behaviors_file = os.path.join(data_path, 'dev', r'behaviors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_col_names = ['ImpressionID', 'UserID', 'Time', 'History', 'Impressions']\n",
    "\n",
    "train_data = pd.read_table(train_behaviors_file, header=None, names=behaviors_col_names)\n",
    "valid_data = pd.read_table(valid_behaviors_file, header=None, names=behaviors_col_names)\n",
    "\n",
    "def data_to_recommended_and_selected(dataframe: pd.DataFrame, user_col: str = 'UserID', interactions_col: str = 'Impressions'):\n",
    "    data = list()\n",
    "    items = set()\n",
    "    for user, user_interactions in zip(dataframe[user_col].values, dataframe[interactions_col].values):\n",
    "        \n",
    "        # Interações são da forma noticia_id-clicado, exemplo: N23699-0 N21291-0 N1901-1 N27292-0 N17443-0\n",
    "\n",
    "        R = [interaction.split('-')[0] for interaction in user_interactions.split()] \n",
    "        S = [interaction.split('-')[0] for interaction in user_interactions.split() if interaction[-1] == '1']\n",
    "        \n",
    "        items.update(R)\n",
    "\n",
    "        data.append((user, ' '.join(R), ' '.join(S)))\n",
    "\n",
    "    return pd.DataFrame(data, columns=['UserID', 'R', 'S']), items\n",
    "\n",
    "train_data, train_items = data_to_recommended_and_selected(train_data)\n",
    "valid_data, valid_items = data_to_recommended_and_selected(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['R'].apply(lambda x: len(x.split(' '))).plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst_s = train_data['R'].apply(lambda x: len(x.split(' ')))\n",
    "tst_s[tst_s <= 10].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['S'].apply(lambda x: len(x.split(' '))).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    149116.000000\n",
       "mean          1.506827\n",
       "std           1.153562\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           1.000000\n",
       "75%           2.000000\n",
       "max          35.000000\n",
       "Name: S, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['S'].apply(lambda x: len(x.split(' '))).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nka72RoX8w2Z"
   },
   "source": [
    "# $P_S$ TEst\n",
    "- Criar o modelo pytorch\n",
    "- Entender como computar a função de perda e treinar\n",
    "\n",
    "Talvez usar máscara para dizer quais elementos estão em R e S.\n",
    "\n",
    "Camada de embedding:\n",
    "- Transformar itens e usuários em indices inteiros.\n",
    "\n",
    "\n",
    "Loss:\n",
    "- Entrada: \n",
    "    - batch com saídas da rede\n",
    "    - Mascara target => indica quais elementos de K estão na mascara.\n",
    "- Reduce => media => é a nivel de batch\n",
    "- Produto interno saída x mascara e soma.\n",
    "\n",
    "\n",
    "Possíveis jeitos de passar as entradas:\n",
    "- 1:\n",
    "    - X: NxK: \n",
    "    - y: NxK: mascara. 1 se item foi escolhido e 0 cc.\n",
    "- 2:\n",
    "    - Dataset cospe dicionarios {u, r, s} ou tuplas (u, r, s)\n",
    "    - collate_fn junta essas entradas em um tensor com padding.\n",
    "\n",
    "\n",
    "Tomar cuidado para não maximizar uma função que é de minimizar ou o contrário.\n",
    "\n",
    "\n",
    "Checar tamanho de K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "4B2wBzkT-IOn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "_GSuBtEHIZBP"
   },
   "outputs": [],
   "source": [
    "def gen_code_dict(code_l):\n",
    "    #gen_code_dict(list(df['UserID'].unique()))\n",
    "    #gen_code_dict(\" \".join(df['R']).split(\" \"))\n",
    "    code_d = {}\n",
    "    i = 1 # 0 eh PADDING\n",
    "    for code in set(code_l):\n",
    "        code_d[code] = i\n",
    "        i += 1\n",
    "    return code_d\n",
    "    \n",
    "class MIND_dataset(Dataset):\n",
    "    def get_picked_inds(self, r, s):\n",
    "        ind_l = []\n",
    "        for picked_i in s:\n",
    "            for ind, i in enumerate(r):\n",
    "                if i == picked_i:\n",
    "                    ind_l.append(ind)\n",
    "                    break\n",
    "        return ind_l\n",
    "\n",
    "\n",
    "    def __init__(self, df, user_d, item_d):\n",
    "        \"\"\"\n",
    "        Recebe um DataFrame com as colunas ['UserID', 'R', 'S']\n",
    "        UserID: ids de usuario\n",
    "        R: listas de ids de itens recomendados a U\n",
    "        S: listas de ids de itens de R escolhidos por U\n",
    "        \n",
    "        user_d: dicionario que mapeia de id de usuário para numero inteiro, sendo que 0 é PAD\n",
    "        item_d: dicionario que mapeia de id de item para numero inteiro, sendo que 0 é PAD\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Transformar os codigos de item e usuario em indices inteiros. 0 eh PADDING\n",
    "        self.user_d = user_d\n",
    "        self.item_d = item_d\n",
    "\n",
    "        self.df = pd.DataFrame(index=df.index)\n",
    "        self.df['U'] = df['UserID'].apply(lambda x: self.user_d[x])\n",
    "        self.df['R'] = df['R'].apply(lambda x: [self.item_d[i] for i in x.split(' ')])\n",
    "        # S\n",
    "        self.df['S_picked'] = df['S'].apply(lambda x: [self.item_d[i] for i in x.split(' ')])\n",
    "        self.df['S'] = self.df.apply(lambda x: self.get_picked_inds(x['R'], x['S_picked']), axis=1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retorna uma tupla com U, R e S\n",
    "        U => id do usuário\n",
    "        R => lista de ids de itens\n",
    "        S => lista de indices de R indicando quais itens foram escolhidos.\n",
    "        '''\n",
    "        return self.df['U'].iloc[idx], self.df['R'].iloc[idx], self.df['S'].iloc[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_n_users(self):\n",
    "        return len(self.user_d)+1\n",
    "\n",
    "    def get_n_items(self):\n",
    "        return len(self.item_d)+1\n",
    "\n",
    "def mind_collate_fn(data):\n",
    "    '''\n",
    "    Retorna tupla: u, r, r_mask, s\n",
    "    '''\n",
    "    batch_sz = len(data)\n",
    "    max_len = max([len(r) for _,r,_ in data])\n",
    "\n",
    "    u_batch = torch.zeros(batch_sz, 1, dtype=torch.long)\n",
    "    r_batch = torch.zeros(batch_sz, max_len, dtype=torch.long)\n",
    "    r_mask_batch = torch.zeros(batch_sz, max_len, dtype=torch.long)\n",
    "    s_batch = torch.zeros(batch_sz, max_len, dtype=torch.long)\n",
    "\n",
    "    for i, (u,r,s) in enumerate(data):\n",
    "        u_batch[i] = u\n",
    "        r_batch[i] = torch.LongTensor(r + [0]*(max_len-len(r)))\n",
    "        r_mask_batch[i] = torch.LongTensor([1]*len(r) + [0]*(max_len-len(r)))\n",
    "        s_batch[i] = torch.LongTensor([(1 if ind in s else 0) for ind in range(max_len)])\n",
    "    return u_batch, r_batch, r_mask_batch, s_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class MIND_P_R_Dataset(Dataset):\n",
    "    def __init__(self, mind_ds, max_sampling_len):\n",
    "        \"\"\"\n",
    "        Retorna amostras com negative sampling.\n",
    "        mind_ds: MIND_Dataset\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mind_ds = mind_ds\n",
    "        self.item_set = set(mind_ds.item_d.values())\n",
    "        self.max_sampling_len = max_sampling_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Retorna uma tupla com U, R_pos, R_neg\n",
    "        U => id do usuário\n",
    "        R_pos => lista de ids de itens em r para U amostrados\n",
    "        R_neg => lista de ids de itens não em r para U amostrados\n",
    "        '''\n",
    "        u,r,_ = self.mind_ds[idx]\n",
    "        pos_r = random.sample(r, min(self.max_sampling_len, len(r)))\n",
    "        neg_r = random.sample(self.item_set - set(r), len(pos_r))\n",
    "        return u, pos_r, neg_r\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mind_ds)\n",
    "    \n",
    "def mind_p_r_collate_fn(data):\n",
    "    '''\n",
    "    Retorna tupla: u, pos_r, pos_r_mask, neg_r, neg_r_mask\n",
    "    '''\n",
    "    batch_sz = len(data)\n",
    "    pos_max_len = max([len(pos_r) for _,pos_r,_ in data])\n",
    "    neg_max_len = max([len(neg_r) for _,_,neg_r in data])\n",
    "\n",
    "    u_batch = torch.zeros(batch_sz, 1, dtype=torch.long)\n",
    "    pos_r_batch = torch.zeros(batch_sz, pos_max_len, dtype=torch.long)\n",
    "    pos_r_mask_batch = torch.zeros(batch_sz, pos_max_len, dtype=torch.long)\n",
    "    neg_r_batch = torch.zeros(batch_sz, neg_max_len, dtype=torch.long)\n",
    "    neg_r_mask_batch = torch.zeros(batch_sz, neg_max_len, dtype=torch.long)\n",
    "\n",
    "    for i, (u, pos_r, neg_r) in enumerate(data):\n",
    "        u_batch[i] = u\n",
    "        pos_r_batch[i] = torch.LongTensor(pos_r + [0]*(pos_max_len-len(pos_r)))\n",
    "        pos_r_mask_batch[i] = torch.LongTensor([1]*len(pos_r) + [0]*(pos_max_len-len(pos_r)))\n",
    "        neg_r_batch[i] = torch.LongTensor(neg_r + [0]*(neg_max_len-len(neg_r)))\n",
    "        neg_r_mask_batch[i] = torch.LongTensor([1]*len(neg_r) + [0]*(neg_max_len-len(neg_r)))\n",
    "        \n",
    "    return u_batch, pos_r_batch, pos_r_mask_batch, neg_r_batch, neg_r_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "BwatALeiYNHR"
   },
   "outputs": [],
   "source": [
    "class P_S_Network(nn.Module):\n",
    "    def __init__(self, n_users, n_itens, K, emb_dim=32):\n",
    "        super(P_S_Network, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_embeddings=n_users, embedding_dim=emb_dim, padding_idx=0)\n",
    "        self.item_emb = nn.Embedding(num_embeddings=n_itens, embedding_dim=emb_dim, padding_idx=0)\n",
    "        self.w = nn.Parameter(torch.randn(1, K))\n",
    "        self.K = K\n",
    "        \n",
    "    def forward(self, u, r, r_mask, s, beta):\n",
    "        \"\"\"\n",
    "        Entrada:\n",
    "            u: Indice do usuário\n",
    "                torch.LongTensor(batch_sz, 1)\n",
    "            r: Tensor de indices dos itens recomendados ao usuário\n",
    "                torch.LongTensor(batch_sz, max_len)\n",
    "            r_mask: Máscara binária indicando quais elementos de r não são padding\n",
    "                torch.LongTensor(batch_sz, max_len)\n",
    "            s: Máscara binária indicando quais elementos de r estão em s.\n",
    "                torch.LongTensor(batch_sz, max_len)\n",
    "            beta: Variavel exogena correspondente aos itens em r\n",
    "                torch.tensor(batch_sz, max_len)\n",
    "        \"\"\"\n",
    "        max_len = r.size(1)\n",
    "        w_mul = beta * (self.w[:,:max_len])\n",
    "\n",
    "        u_e = self.user_emb(u)\n",
    "        i_e = self.item_emb(r)\n",
    "        emb_sim = torch.bmm(u_e, i_e.permute(0,2,1)).squeeze()\n",
    "\n",
    "        return (w_mul + emb_sim) * r_mask\n",
    "\n",
    "def p_s_loss(out, s):\n",
    "    # Loss to be minimized\n",
    "    out_soft = F.log_softmax(out, dim=1)\n",
    "    return -(out_soft*s).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_R_Network(nn.Module):\n",
    "    def __init__(self, n_users, n_itens, emb_dim=32):\n",
    "        super(P_R_Network, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_embeddings=n_users, embedding_dim=emb_dim, padding_idx=0)\n",
    "        self.item_emb = nn.Embedding(num_embeddings=n_itens, embedding_dim=emb_dim, padding_idx=0)\n",
    "        self.w = nn.Parameter(torch.randn(1, n_itens))\n",
    "        \n",
    "    def forward(self, u, r, r_mask, alpha):\n",
    "        \"\"\"\n",
    "        Entrada:\n",
    "            u: Indice do usuário\n",
    "                torch.LongTensor(batch_sz, 1)\n",
    "            r: Tensor de indices dos itens recomendados ao usuário\n",
    "                torch.LongTensor(batch_sz, max_len)\n",
    "            r_mask: Máscara binária indicando quais elementos de r não são padding\n",
    "                torch.LongTensor(batch_sz, max_len)\n",
    "            alpha: Variavel exogena correspondente aos itens em r\n",
    "                torch.tensor(batch_sz, max_len)\n",
    "        \"\"\"\n",
    "        # Seleciona os w correspondentes aos itens em r\n",
    "        w_mul = alpha * (self.w[0,r])\n",
    "\n",
    "        u_e = self.user_emb(u)\n",
    "        i_e = self.item_emb(r)\n",
    "        emb_sim = torch.bmm(u_e, i_e.permute(0,2,1)).squeeze()\n",
    "\n",
    "        return (w_mul + emb_sim) * r_mask\n",
    "\n",
    "def p_r_loss(pos_scores, neg_scores):\n",
    "    \"\"\"\n",
    "    Negative sampling loss.\n",
    "    Entrada:\n",
    "        pos_scores: torch.tensor(batch_sz, pos_max_len)\n",
    "        neg_scores: torch.tensor(batch_sz, neg_max_len)\n",
    "    \"\"\"\n",
    "    # Loss to be minimized\n",
    "    eps=1e-7\n",
    "    pos_soft = F.logsigmoid(pos_scores).sum()\n",
    "    neg_soft = torch.log(eps + 1 - torch.sigmoid(neg_scores)).sum()\n",
    "    return -(pos_soft + neg_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "id": "RAvnmUFWvIRA"
   },
   "outputs": [],
   "source": [
    "def ps_train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    agg_loss = 0.0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (u, r, r_mask, s) in enumerate(dataloader):\n",
    "        u, r, r_mask, s = u.to(device), r.to(device), r_mask.to(device), s.to(device)\n",
    "        \n",
    "        batch_sz = u.size(0)\n",
    "        max_len = r.size(1)\n",
    "        beta = torch.randn(batch_sz, max_len).to(device)\n",
    "        out = model(u, r, r_mask, s, beta)\n",
    "        loss = loss_fn(out, s)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        agg_loss += loss.item()\n",
    "        if batch % (len(dataloader)//10) == 0:\n",
    "            loss, current = loss.item() / batch_sz, batch * len(u)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return agg_loss / size\n",
    "\n",
    "def ps_test_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u, r, r_mask, s in dataloader:\n",
    "            u, r, r_mask, s = u.to(device), r.to(device), r_mask.to(device), s.to(device)\n",
    "            \n",
    "            batch_sz = u.size(0)\n",
    "            max_len = r.size(1)\n",
    "            beta = torch.randn(batch_sz, max_len).to(device)\n",
    "            out = model(u, r, r_mask, s, beta)\n",
    "            test_loss += loss_fn(out, s).item() / batch_sz\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    agg_loss = 0.0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (u, pos_r, pos_r_mask, neg_r, neg_r_mask) in enumerate(dataloader):\n",
    "        u, pos_r, pos_r_mask, neg_r, neg_r_mask = u.to(device), pos_r.to(device), pos_r_mask.to(device), neg_r.to(device), neg_r_mask.to(device)\n",
    "        batch_sz = u.size(0)\n",
    "        pos_max_len = pos_r.size(1)\n",
    "        neg_max_len = neg_r.size(1)\n",
    "        \n",
    "        pos_alpha = torch.randn(batch_sz, pos_max_len).to(device)\n",
    "        pos_scores = model(u, pos_r, pos_r_mask, pos_alpha)\n",
    "        \n",
    "        neg_alpha = torch.randn(batch_sz, neg_max_len).to(device)\n",
    "        neg_scores = model(u, neg_r, neg_r_mask, neg_alpha)\n",
    "        \n",
    "        loss = loss_fn(pos_scores, neg_scores)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        agg_loss += loss.item()\n",
    "        if batch % (len(dataloader)//10) == 0:\n",
    "            loss, current = loss.item() / batch_sz, batch * len(u)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return agg_loss / size\n",
    "\n",
    "def pr_test_loop(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u, pos_r, pos_r_mask, neg_r, neg_r_mask in dataloader:\n",
    "            u, pos_r, pos_r_mask, neg_r, neg_r_mask = u.to(device), pos_r.to(device), pos_r_mask.to(device), neg_r.to(device), neg_r_mask.to(device)\n",
    "            batch_sz = u.size(0)\n",
    "            pos_max_len = pos_r.size(1)\n",
    "            neg_max_len = neg_r.size(1)\n",
    "        \n",
    "            pos_alpha = torch.randn(batch_sz, pos_max_len).to(device)\n",
    "            pos_scores = model(u, pos_r, pos_r_mask, pos_alpha)\n",
    "        \n",
    "            neg_alpha = torch.randn(batch_sz, neg_max_len).to(device)\n",
    "            neg_scores = model(u, neg_r, neg_r_mask, neg_alpha)\n",
    "            \n",
    "            test_loss += loss_fn(pos_scores, neg_scores).item() / batch_sz\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treino :  134204\n",
      "Val :  12799\n"
     ]
    }
   ],
   "source": [
    "# Pre-processamento\n",
    "df_ids = list(range(len(train_data)))\n",
    "random.shuffle(df_ids)\n",
    "train_ids = df_ids[:int(len(train_data)*0.9)]\n",
    "val_ids = df_ids[int(len(train_data)*0.9):]\n",
    "\n",
    "train_df = train_data.iloc[train_ids]\n",
    "val_df = train_data.iloc[val_ids]\n",
    "\n",
    "user_d = gen_code_dict(list(train_df['UserID'].unique()))\n",
    "item_d = gen_code_dict(\" \".join(train_df['R']).split(\" \"))\n",
    "\n",
    "item_check_s = val_df.apply(lambda x: x['R'].split() + x['S'].split(), axis=1).apply(lambda l: all([ i in item_d for i in l]))\n",
    "user_check_s = val_df['UserID'].apply(lambda x: x in user_d)\n",
    "val_df = val_df[item_check_s & user_check_s]\n",
    "print('Treino : ', len(train_df))\n",
    "print('Val : ', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar datasets\n",
    "train_df.to_csv('train_df.csv', index=False)\n",
    "val_df.to_csv('val_df.csv', index=False)\n",
    "\n",
    "# Salvar os dicts de usuario e item\n",
    "pd.DataFrame({'code':user_d.keys(), 'indice':user_d.values()}).to_csv('user_d.csv', index=False)\n",
    "pd.DataFrame({'code':item_d.keys(), 'indice':item_d.values()}).to_csv('item_d.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "batch_size = 32\n",
    "p_r_max_sampling_len = 5\n",
    "\n",
    "ps_train_ds = MIND_dataset(train_df, user_d, item_d)\n",
    "ps_train_dl = DataLoader(ps_train_ds, batch_size=batch_size, shuffle=True, collate_fn=mind_collate_fn)\n",
    "\n",
    "ps_val_ds = MIND_dataset(val_df, user_d, item_d)\n",
    "ps_val_dl = DataLoader(ps_val_ds, batch_size=batch_size, shuffle=True, collate_fn=mind_collate_fn)\n",
    "\n",
    "\n",
    "pr_train_ds = MIND_P_R_Dataset(ps_train_ds, p_r_max_sampling_len)\n",
    "pr_train_dl = DataLoader(pr_train_ds, batch_size=batch_size, shuffle=True, collate_fn=mind_p_r_collate_fn)\n",
    "\n",
    "pr_val_ds = MIND_P_R_Dataset(ps_val_ds, p_r_max_sampling_len)\n",
    "pr_val_dl = DataLoader(pr_val_ds, batch_size=batch_size, shuffle=True, collate_fn=mind_p_r_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $P_R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_R:\n",
      "w torch.Size([1, 19762])\n",
      "Cuda =  False\n",
      "user_emb.weight torch.Size([47372, 32])\n",
      "Cuda =  False\n",
      "item_emb.weight torch.Size([19762, 32])\n",
      "Cuda =  False\n"
     ]
    }
   ],
   "source": [
    "pr_model = P_R_Network(len(user_d)+1, len(item_d)+1)\n",
    "pr_model = pr_model.to(device)\n",
    "print('P_R:')     \n",
    "for name, param in pr_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.size())\n",
    "        print('Cuda = ', param.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_save_path = 'pr_model.pth'\n",
    "pr_loss_save_path = 'pr_loss.csv'\n",
    "pr_loss_d = {'epoch': [], 'train':[], 'val':[]}\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(pr_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 11.774590  [    0/134204]\n",
      "loss: 14.734699  [13408/134204]\n",
      "loss: 14.827754  [26816/134204]\n",
      "loss: 11.852610  [40224/134204]\n",
      "loss: 12.114583  [53632/134204]\n",
      "loss: 14.610188  [67040/134204]\n",
      "loss: 12.793890  [80448/134204]\n",
      "loss: 12.699725  [93856/134204]\n",
      "loss: 14.020510  [107264/134204]\n",
      "loss: 13.403305  [120672/134204]\n",
      "loss: 15.822097  [134080/134204]\n",
      "Test Error: Avg loss: 14.249304 \n",
      "\n",
      "--- 122.95591878890991 seconds ---\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 13.024746  [    0/134204]\n",
      "loss: 14.136679  [13408/134204]\n",
      "loss: 16.539394  [26816/134204]\n",
      "loss: 13.459517  [40224/134204]\n",
      "loss: 13.145348  [53632/134204]\n",
      "loss: 11.983621  [67040/134204]\n",
      "loss: 16.224863  [80448/134204]\n",
      "loss: 15.383274  [93856/134204]\n",
      "loss: 13.968799  [107264/134204]\n",
      "loss: 12.487881  [120672/134204]\n",
      "loss: 15.803965  [134080/134204]\n",
      "Test Error: Avg loss: 14.165283 \n",
      "\n",
      "--- 118.01939630508423 seconds ---\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 12.452364  [    0/134204]\n",
      "loss: 14.507866  [13408/134204]\n",
      "loss: 15.382910  [26816/134204]\n",
      "loss: 12.694801  [40224/134204]\n",
      "loss: 15.223822  [53632/134204]\n",
      "loss: 15.840056  [67040/134204]\n",
      "loss: 13.182121  [80448/134204]\n",
      "loss: 14.449609  [93856/134204]\n",
      "loss: 14.770085  [107264/134204]\n",
      "loss: 13.176246  [120672/134204]\n",
      "loss: 13.408873  [134080/134204]\n",
      "Test Error: Avg loss: 14.147254 \n",
      "\n",
      "--- 118.01239085197449 seconds ---\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 15.204929  [    0/134204]\n",
      "loss: 14.068369  [13408/134204]\n",
      "loss: 14.124206  [26816/134204]\n",
      "loss: 15.829858  [40224/134204]\n",
      "loss: 15.012207  [53632/134204]\n",
      "loss: 13.711449  [67040/134204]\n",
      "loss: 15.318354  [80448/134204]\n",
      "loss: 14.283468  [93856/134204]\n",
      "loss: 12.712679  [107264/134204]\n",
      "loss: 15.100167  [120672/134204]\n",
      "loss: 13.219393  [134080/134204]\n",
      "Test Error: Avg loss: 14.104589 \n",
      "\n",
      "--- 122.63372230529785 seconds ---\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 14.296484  [    0/134204]\n",
      "loss: 13.663281  [13408/134204]\n",
      "loss: 16.267128  [26816/134204]\n",
      "loss: 13.057612  [40224/134204]\n",
      "loss: 13.567505  [53632/134204]\n",
      "loss: 13.912495  [67040/134204]\n",
      "loss: 12.122295  [80448/134204]\n",
      "loss: 13.832640  [93856/134204]\n",
      "loss: 14.948818  [107264/134204]\n",
      "loss: 14.212776  [120672/134204]\n",
      "loss: 13.592257  [134080/134204]\n",
      "Test Error: Avg loss: 14.161218 \n",
      "\n",
      "--- 122.34750580787659 seconds ---\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 12.698524  [    0/134204]\n",
      "loss: 15.443617  [13408/134204]\n",
      "loss: 16.016853  [26816/134204]\n",
      "loss: 15.101542  [40224/134204]\n",
      "loss: 16.170429  [53632/134204]\n",
      "loss: 12.719382  [67040/134204]\n",
      "loss: 16.847597  [80448/134204]\n",
      "loss: 15.086264  [93856/134204]\n",
      "loss: 12.510916  [107264/134204]\n",
      "loss: 13.588921  [120672/134204]\n",
      "loss: 15.010941  [134080/134204]\n",
      "Test Error: Avg loss: 14.037214 \n",
      "\n",
      "--- 123.19467115402222 seconds ---\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 14.741800  [    0/134204]\n",
      "loss: 14.106444  [13408/134204]\n",
      "loss: 14.660498  [26816/134204]\n",
      "loss: 12.094316  [40224/134204]\n",
      "loss: 14.391708  [53632/134204]\n",
      "loss: 13.443050  [67040/134204]\n",
      "loss: 13.576177  [80448/134204]\n",
      "loss: 13.651770  [93856/134204]\n",
      "loss: 13.812412  [107264/134204]\n",
      "loss: 13.009331  [120672/134204]\n",
      "loss: 14.410403  [134080/134204]\n",
      "Test Error: Avg loss: 13.946173 \n",
      "\n",
      "--- 129.31468892097473 seconds ---\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 12.959032  [    0/134204]\n",
      "loss: 15.360010  [13408/134204]\n",
      "loss: 11.967000  [26816/134204]\n",
      "loss: 12.491609  [40224/134204]\n",
      "loss: 13.302027  [53632/134204]\n",
      "loss: 12.272827  [67040/134204]\n",
      "loss: 12.321503  [80448/134204]\n",
      "loss: 15.211381  [93856/134204]\n",
      "loss: 13.762728  [107264/134204]\n",
      "loss: 12.934961  [120672/134204]\n",
      "loss: 13.698498  [134080/134204]\n",
      "Test Error: Avg loss: 13.936960 \n",
      "\n",
      "--- 127.97467589378357 seconds ---\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 11.990852  [    0/134204]\n",
      "loss: 13.717438  [13408/134204]\n",
      "loss: 12.873127  [26816/134204]\n",
      "loss: 13.699385  [40224/134204]\n",
      "loss: 13.150156  [53632/134204]\n",
      "loss: 12.669086  [67040/134204]\n",
      "loss: 12.673268  [80448/134204]\n",
      "loss: 10.498484  [93856/134204]\n",
      "loss: 14.530880  [107264/134204]\n",
      "loss: 14.484200  [120672/134204]\n",
      "loss: 14.090147  [134080/134204]\n",
      "Test Error: Avg loss: 13.793983 \n",
      "\n",
      "--- 116.81582975387573 seconds ---\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 12.900328  [    0/134204]\n",
      "loss: 13.961847  [13408/134204]\n",
      "loss: 14.579906  [26816/134204]\n",
      "loss: 13.829674  [40224/134204]\n",
      "loss: 12.592329  [53632/134204]\n",
      "loss: 12.863193  [67040/134204]\n",
      "loss: 14.674153  [80448/134204]\n",
      "loss: 13.421968  [93856/134204]\n",
      "loss: 13.939850  [107264/134204]\n",
      "loss: 13.248922  [120672/134204]\n",
      "loss: 14.301275  [134080/134204]\n",
      "Test Error: Avg loss: 13.744462 \n",
      "\n",
      "--- 123.54506206512451 seconds ---\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 12.234909  [    0/134204]\n",
      "loss: 10.648510  [13408/134204]\n",
      "loss: 13.543106  [26816/134204]\n",
      "loss: 14.143044  [40224/134204]\n",
      "loss: 14.380263  [53632/134204]\n",
      "loss: 12.990129  [67040/134204]\n",
      "loss: 11.803049  [80448/134204]\n",
      "loss: 12.947014  [93856/134204]\n",
      "loss: 12.486638  [107264/134204]\n",
      "loss: 14.214203  [120672/134204]\n",
      "loss: 12.110312  [134080/134204]\n",
      "Test Error: Avg loss: 13.615832 \n",
      "\n",
      "--- 128.09478306770325 seconds ---\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 12.700643  [    0/134204]\n",
      "loss: 13.189139  [13408/134204]\n",
      "loss: 11.574539  [26816/134204]\n",
      "loss: 15.044923  [40224/134204]\n",
      "loss: 13.273588  [53632/134204]\n",
      "loss: 14.112013  [67040/134204]\n",
      "loss: 14.268091  [80448/134204]\n",
      "loss: 12.720024  [93856/134204]\n",
      "loss: 12.775608  [107264/134204]\n",
      "loss: 13.155431  [120672/134204]\n",
      "loss: 13.767597  [134080/134204]\n",
      "Test Error: Avg loss: 13.675079 \n",
      "\n",
      "--- 131.18274688720703 seconds ---\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 13.830004  [    0/134204]\n",
      "loss: 11.436213  [13408/134204]\n",
      "loss: 14.061310  [26816/134204]\n",
      "loss: 13.168753  [40224/134204]\n",
      "loss: 13.056964  [53632/134204]\n",
      "loss: 12.293750  [67040/134204]\n",
      "loss: 13.044100  [80448/134204]\n",
      "loss: 12.866230  [93856/134204]\n",
      "loss: 12.278913  [107264/134204]\n",
      "loss: 11.765852  [120672/134204]\n",
      "loss: 13.769907  [134080/134204]\n",
      "Test Error: Avg loss: 13.680582 \n",
      "\n",
      "--- 123.14901614189148 seconds ---\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 12.812127  [    0/134204]\n",
      "loss: 13.984259  [13408/134204]\n",
      "loss: 13.380793  [26816/134204]\n",
      "loss: 13.082788  [40224/134204]\n",
      "loss: 12.355531  [53632/134204]\n",
      "loss: 12.938145  [67040/134204]\n",
      "loss: 12.303051  [80448/134204]\n",
      "loss: 12.783318  [93856/134204]\n",
      "loss: 12.119774  [107264/134204]\n",
      "loss: 14.402069  [120672/134204]\n",
      "loss: 12.884599  [134080/134204]\n",
      "Test Error: Avg loss: 13.586084 \n",
      "\n",
      "--- 122.74606990814209 seconds ---\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 12.718431  [    0/134204]\n",
      "loss: 15.577252  [13408/134204]\n",
      "loss: 11.880184  [26816/134204]\n",
      "loss: 13.007774  [40224/134204]\n",
      "loss: 12.717389  [53632/134204]\n",
      "loss: 12.294562  [67040/134204]\n",
      "loss: 13.737033  [80448/134204]\n",
      "loss: 12.181681  [93856/134204]\n",
      "loss: 12.061831  [107264/134204]\n",
      "loss: 12.807624  [120672/134204]\n",
      "loss: 13.061156  [134080/134204]\n",
      "Test Error: Avg loss: 13.623701 \n",
      "\n",
      "--- 117.04049515724182 seconds ---\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 11.829892  [    0/134204]\n",
      "loss: 13.030218  [13408/134204]\n",
      "loss: 11.740548  [26816/134204]\n",
      "loss: 12.977193  [40224/134204]\n",
      "loss: 11.952809  [53632/134204]\n",
      "loss: 14.266560  [67040/134204]\n",
      "loss: 14.383896  [80448/134204]\n",
      "loss: 13.704786  [93856/134204]\n",
      "loss: 13.524942  [107264/134204]\n",
      "loss: 12.885172  [120672/134204]\n",
      "loss: 13.358950  [134080/134204]\n",
      "Test Error: Avg loss: 13.458409 \n",
      "\n",
      "--- 116.4474868774414 seconds ---\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 10.917913  [    0/134204]\n",
      "loss: 13.193300  [13408/134204]\n",
      "loss: 11.059986  [26816/134204]\n",
      "loss: 14.681410  [40224/134204]\n",
      "loss: 14.964281  [53632/134204]\n",
      "loss: 13.658060  [67040/134204]\n",
      "loss: 13.146675  [80448/134204]\n",
      "loss: 12.719145  [93856/134204]\n",
      "loss: 14.607900  [107264/134204]\n",
      "loss: 12.473481  [120672/134204]\n",
      "loss: 13.488994  [134080/134204]\n",
      "Test Error: Avg loss: 13.346117 \n",
      "\n",
      "--- 114.87055897712708 seconds ---\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 14.442728  [    0/134204]\n",
      "loss: 12.206697  [13408/134204]\n",
      "loss: 14.439173  [26816/134204]\n",
      "loss: 13.731812  [40224/134204]\n",
      "loss: 12.407985  [53632/134204]\n",
      "loss: 12.513251  [67040/134204]\n",
      "loss: 12.602093  [80448/134204]\n",
      "loss: 10.617551  [93856/134204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 13.385911  [107264/134204]\n",
      "loss: 11.487513  [120672/134204]\n",
      "loss: 13.383480  [134080/134204]\n",
      "Test Error: Avg loss: 13.352102 \n",
      "\n",
      "--- 122.6266417503357 seconds ---\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 13.906094  [    0/134204]\n",
      "loss: 13.255186  [13408/134204]\n",
      "loss: 11.867412  [26816/134204]\n",
      "loss: 12.989321  [40224/134204]\n",
      "loss: 13.288486  [53632/134204]\n",
      "loss: 12.941021  [67040/134204]\n",
      "loss: 13.457550  [80448/134204]\n",
      "loss: 13.047547  [93856/134204]\n",
      "loss: 12.794102  [107264/134204]\n",
      "loss: 13.207448  [120672/134204]\n",
      "loss: 12.542682  [134080/134204]\n",
      "Test Error: Avg loss: 13.302059 \n",
      "\n",
      "--- 122.45627427101135 seconds ---\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 11.797218  [    0/134204]\n",
      "loss: 13.809884  [13408/134204]\n",
      "loss: 12.806397  [26816/134204]\n",
      "loss: 12.998087  [40224/134204]\n",
      "loss: 13.667526  [53632/134204]\n",
      "loss: 12.300240  [67040/134204]\n",
      "loss: 14.650997  [80448/134204]\n",
      "loss: 13.748977  [93856/134204]\n",
      "loss: 14.227724  [107264/134204]\n",
      "loss: 11.846514  [120672/134204]\n",
      "loss: 12.516551  [134080/134204]\n",
      "Test Error: Avg loss: 13.255329 \n",
      "\n",
      "--- 117.69599485397339 seconds ---\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 11.921545  [    0/134204]\n",
      "loss: 15.056828  [13408/134204]\n",
      "loss: 11.060843  [26816/134204]\n",
      "loss: 12.696754  [40224/134204]\n",
      "loss: 10.851971  [53632/134204]\n",
      "loss: 14.574313  [67040/134204]\n",
      "loss: 14.697881  [80448/134204]\n",
      "loss: 13.866623  [93856/134204]\n",
      "loss: 14.496576  [107264/134204]\n",
      "loss: 16.345118  [120672/134204]\n",
      "loss: 13.530753  [134080/134204]\n",
      "Test Error: Avg loss: 13.143056 \n",
      "\n",
      "--- 122.50948405265808 seconds ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 12.983795  [    0/134204]\n",
      "loss: 13.365521  [13408/134204]\n",
      "loss: 13.732771  [26816/134204]\n",
      "loss: 12.943107  [40224/134204]\n",
      "loss: 13.354784  [53632/134204]\n",
      "loss: 13.635418  [67040/134204]\n",
      "loss: 12.849874  [80448/134204]\n",
      "loss: 14.126583  [93856/134204]\n",
      "loss: 14.600001  [107264/134204]\n",
      "loss: 12.148489  [120672/134204]\n",
      "loss: 15.101410  [134080/134204]\n",
      "Test Error: Avg loss: 13.140567 \n",
      "\n",
      "--- 122.10985398292542 seconds ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 13.657070  [    0/134204]\n",
      "loss: 13.421770  [13408/134204]\n",
      "loss: 12.058612  [26816/134204]\n",
      "loss: 11.798140  [40224/134204]\n",
      "loss: 10.948601  [53632/134204]\n",
      "loss: 13.235999  [67040/134204]\n",
      "loss: 12.877935  [80448/134204]\n",
      "loss: 12.877302  [93856/134204]\n",
      "loss: 14.702676  [107264/134204]\n",
      "loss: 11.161775  [120672/134204]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2y/kyv2hdc1433f67fbbd7zm9zr0000gn/T/ipykernel_44627/1359026164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch_n+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_r_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpr_test_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_val_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpr_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_r_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2y/kyv2hdc1433f67fbbd7zm9zr0000gn/T/ipykernel_44627/2424618608.py\u001b[0m in \u001b[0;36mpr_train_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0magg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2y/kyv2hdc1433f67fbbd7zm9zr0000gn/T/ipykernel_44627/1543202309.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmind_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpos_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sampling_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mneg_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_set\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch = 40\n",
    "end_epoch = 80\n",
    "\n",
    "for epoch_n in range(start_epoch, end_epoch):\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch {epoch_n+1}\\n-------------------------------\")\n",
    "    train_loss = pr_train_loop(pr_train_dl, pr_model, p_r_loss, optimizer, device)\n",
    "    val_loss = pr_test_loop(pr_val_dl, pr_model, p_r_loss, device)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    pr_loss_d['epoch'].append(epoch_n)\n",
    "    pr_loss_d['train'].append(train_loss)\n",
    "    pr_loss_d['val'].append(val_loss)\n",
    "    if epoch_n % 3 == 0:\n",
    "        torch.save(pr_model.state_dict(), pr_save_path)\n",
    "        pd.DataFrame(pr_loss_d).to_csv(pr_loss_save_path, index=False)\n",
    "torch.save(pr_model.state_dict(), pr_save_path)\n",
    "pd.DataFrame(pr_loss_d).to_csv(pr_loss_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pr_model = P_R_Network(len(user_d)+1, len(item_d)+1)\n",
    "loaded_pr_model.load_state_dict(torch.load(pr_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $P_S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_S:\n",
      "w torch.Size([1, 299])\n",
      "Cuda =  False\n",
      "user_emb.weight torch.Size([47372, 32])\n",
      "Cuda =  False\n",
      "item_emb.weight torch.Size([19762, 32])\n",
      "Cuda =  False\n"
     ]
    }
   ],
   "source": [
    "ps_model = P_S_Network(len(user_d)+1, len(item_d)+1, 299)\n",
    "ps_model = ps_model.to(device)\n",
    "# Parametros\n",
    "print('P_S:')\n",
    "for name, param in ps_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.size())\n",
    "        print('Cuda = ', param.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_save_path = 'ps_model.pth'\n",
    "ps_loss_save_path = 'ps_loss.csv'\n",
    "ps_loss_d = {'epoch': [], 'train':[], 'val':[]}\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(ps_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 7.089635  [    0/134204]\n",
      "loss: 7.554595  [13408/134204]\n",
      "loss: 8.646999  [26816/134204]\n",
      "loss: 6.770673  [40224/134204]\n",
      "loss: 7.537249  [53632/134204]\n",
      "loss: 7.435784  [67040/134204]\n",
      "loss: 7.798216  [80448/134204]\n",
      "loss: 6.145470  [93856/134204]\n",
      "loss: 6.150430  [107264/134204]\n",
      "loss: 7.486312  [120672/134204]\n",
      "loss: 7.144565  [134080/134204]\n",
      "Test Error: Avg loss: 9.854583 \n",
      "\n",
      "--- 38.799513816833496 seconds ---\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 6.541159  [    0/134204]\n",
      "loss: 8.108786  [13408/134204]\n",
      "loss: 6.930912  [26816/134204]\n",
      "loss: 8.062670  [40224/134204]\n",
      "loss: 9.675079  [53632/134204]\n",
      "loss: 7.900226  [67040/134204]\n",
      "loss: 6.713017  [80448/134204]\n",
      "loss: 9.597444  [93856/134204]\n",
      "loss: 8.245149  [107264/134204]\n",
      "loss: 7.077670  [120672/134204]\n",
      "loss: 6.819654  [134080/134204]\n",
      "Test Error: Avg loss: 9.840430 \n",
      "\n",
      "--- 46.16293478012085 seconds ---\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 5.735857  [    0/134204]\n",
      "loss: 6.988407  [13408/134204]\n",
      "loss: 5.393403  [26816/134204]\n",
      "loss: 6.222525  [40224/134204]\n",
      "loss: 5.452482  [53632/134204]\n",
      "loss: 9.659222  [67040/134204]\n",
      "loss: 8.634267  [80448/134204]\n",
      "loss: 7.151680  [93856/134204]\n",
      "loss: 7.153098  [107264/134204]\n",
      "loss: 7.801962  [120672/134204]\n",
      "loss: 7.487266  [134080/134204]\n",
      "Test Error: Avg loss: 9.824826 \n",
      "\n",
      "--- 40.18994188308716 seconds ---\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 6.424115  [    0/134204]\n",
      "loss: 6.717396  [13408/134204]\n",
      "loss: 7.416263  [26816/134204]\n",
      "loss: 8.551511  [40224/134204]\n",
      "loss: 8.753600  [53632/134204]\n",
      "loss: 6.668077  [67040/134204]\n",
      "loss: 7.548347  [80448/134204]\n",
      "loss: 6.258226  [93856/134204]\n",
      "loss: 9.352469  [107264/134204]\n",
      "loss: 7.299218  [120672/134204]\n",
      "loss: 5.877839  [134080/134204]\n",
      "Test Error: Avg loss: 9.800791 \n",
      "\n",
      "--- 40.382392168045044 seconds ---\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 8.457439  [    0/134204]\n",
      "loss: 5.451632  [13408/134204]\n",
      "loss: 10.524567  [26816/134204]\n",
      "loss: 7.579388  [40224/134204]\n",
      "loss: 5.676372  [53632/134204]\n",
      "loss: 6.953359  [67040/134204]\n",
      "loss: 9.570496  [80448/134204]\n",
      "loss: 9.155666  [93856/134204]\n",
      "loss: 6.075233  [107264/134204]\n",
      "loss: 7.903865  [120672/134204]\n",
      "loss: 6.566817  [134080/134204]\n",
      "Test Error: Avg loss: 9.791539 \n",
      "\n",
      "--- 38.70849895477295 seconds ---\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 9.806322  [    0/134204]\n",
      "loss: 7.095025  [13408/134204]\n",
      "loss: 8.528683  [26816/134204]\n",
      "loss: 7.536183  [40224/134204]\n",
      "loss: 8.486618  [53632/134204]\n",
      "loss: 5.768966  [67040/134204]\n",
      "loss: 8.657160  [80448/134204]\n",
      "loss: 5.497946  [93856/134204]\n",
      "loss: 8.187733  [107264/134204]\n",
      "loss: 6.590748  [120672/134204]\n",
      "loss: 6.710081  [134080/134204]\n",
      "Test Error: Avg loss: 9.775924 \n",
      "\n",
      "--- 44.8751757144928 seconds ---\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 8.301575  [    0/134204]\n",
      "loss: 10.430965  [13408/134204]\n",
      "loss: 5.476646  [26816/134204]\n",
      "loss: 7.016807  [40224/134204]\n",
      "loss: 7.049194  [53632/134204]\n",
      "loss: 6.404098  [67040/134204]\n",
      "loss: 6.354930  [80448/134204]\n",
      "loss: 7.855061  [93856/134204]\n",
      "loss: 6.882784  [107264/134204]\n",
      "loss: 6.062992  [120672/134204]\n",
      "loss: 6.546295  [134080/134204]\n",
      "Test Error: Avg loss: 9.765177 \n",
      "\n",
      "--- 42.15120601654053 seconds ---\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 4.970705  [    0/134204]\n",
      "loss: 5.255832  [13408/134204]\n",
      "loss: 6.149053  [26816/134204]\n",
      "loss: 8.173880  [40224/134204]\n",
      "loss: 8.905376  [53632/134204]\n",
      "loss: 8.898103  [67040/134204]\n",
      "loss: 8.082692  [80448/134204]\n",
      "loss: 8.136092  [93856/134204]\n",
      "loss: 7.293880  [107264/134204]\n",
      "loss: 6.957293  [120672/134204]\n",
      "loss: 7.189439  [134080/134204]\n",
      "Test Error: Avg loss: 9.745827 \n",
      "\n",
      "--- 40.60179400444031 seconds ---\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 8.406008  [    0/134204]\n",
      "loss: 7.965212  [13408/134204]\n",
      "loss: 6.125977  [26816/134204]\n",
      "loss: 8.299169  [40224/134204]\n",
      "loss: 6.393892  [53632/134204]\n",
      "loss: 5.314594  [67040/134204]\n",
      "loss: 7.126779  [80448/134204]\n",
      "loss: 6.550650  [93856/134204]\n",
      "loss: 7.073471  [107264/134204]\n",
      "loss: 5.730305  [120672/134204]\n",
      "loss: 7.203839  [134080/134204]\n",
      "Test Error: Avg loss: 9.725757 \n",
      "\n",
      "--- 37.167827129364014 seconds ---\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 8.958080  [    0/134204]\n",
      "loss: 6.234895  [13408/134204]\n",
      "loss: 6.753739  [26816/134204]\n",
      "loss: 8.413649  [40224/134204]\n",
      "loss: 6.456336  [53632/134204]\n",
      "loss: 5.838299  [67040/134204]\n",
      "loss: 7.091902  [80448/134204]\n",
      "loss: 6.079276  [93856/134204]\n",
      "loss: 7.496367  [107264/134204]\n",
      "loss: 7.394235  [120672/134204]\n",
      "loss: 6.899639  [134080/134204]\n",
      "Test Error: Avg loss: 9.719134 \n",
      "\n",
      "--- 40.32427477836609 seconds ---\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 10.276801  [    0/134204]\n",
      "loss: 6.673552  [13408/134204]\n",
      "loss: 6.858521  [26816/134204]\n",
      "loss: 6.423937  [40224/134204]\n",
      "loss: 8.069423  [53632/134204]\n",
      "loss: 5.680932  [67040/134204]\n",
      "loss: 6.593703  [80448/134204]\n",
      "loss: 6.227246  [93856/134204]\n",
      "loss: 7.560793  [107264/134204]\n",
      "loss: 5.576988  [120672/134204]\n",
      "loss: 9.595158  [134080/134204]\n",
      "Test Error: Avg loss: 9.710459 \n",
      "\n",
      "--- 38.68807101249695 seconds ---\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 6.521454  [    0/134204]\n",
      "loss: 7.277666  [13408/134204]\n",
      "loss: 8.331271  [26816/134204]\n",
      "loss: 7.005593  [40224/134204]\n",
      "loss: 7.690495  [53632/134204]\n",
      "loss: 7.334070  [67040/134204]\n",
      "loss: 9.889481  [80448/134204]\n",
      "loss: 6.934512  [93856/134204]\n",
      "loss: 8.759684  [107264/134204]\n",
      "loss: 7.493528  [120672/134204]\n",
      "loss: 6.872874  [134080/134204]\n",
      "Test Error: Avg loss: 9.689951 \n",
      "\n",
      "--- 40.22892999649048 seconds ---\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 6.492934  [    0/134204]\n",
      "loss: 6.228429  [13408/134204]\n",
      "loss: 7.766979  [26816/134204]\n",
      "loss: 6.707466  [40224/134204]\n",
      "loss: 6.450001  [53632/134204]\n",
      "loss: 6.409388  [67040/134204]\n",
      "loss: 6.541497  [80448/134204]\n",
      "loss: 5.062025  [93856/134204]\n",
      "loss: 6.806691  [107264/134204]\n",
      "loss: 7.330394  [120672/134204]\n",
      "loss: 5.560602  [134080/134204]\n",
      "Test Error: Avg loss: 9.673204 \n",
      "\n",
      "--- 36.48459815979004 seconds ---\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 6.950807  [    0/134204]\n",
      "loss: 6.640628  [13408/134204]\n",
      "loss: 6.227753  [26816/134204]\n",
      "loss: 6.903853  [40224/134204]\n",
      "loss: 5.978732  [53632/134204]\n",
      "loss: 7.096800  [67040/134204]\n",
      "loss: 8.117173  [80448/134204]\n",
      "loss: 5.659278  [93856/134204]\n",
      "loss: 6.762105  [107264/134204]\n",
      "loss: 8.392741  [120672/134204]\n",
      "loss: 7.079730  [134080/134204]\n",
      "Test Error: Avg loss: 9.678551 \n",
      "\n",
      "--- 38.38912510871887 seconds ---\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 6.811863  [    0/134204]\n",
      "loss: 8.223378  [13408/134204]\n",
      "loss: 10.270326  [26816/134204]\n",
      "loss: 7.280582  [40224/134204]\n",
      "loss: 8.620399  [53632/134204]\n",
      "loss: 8.631713  [67040/134204]\n",
      "loss: 6.883114  [80448/134204]\n",
      "loss: 5.981043  [93856/134204]\n",
      "loss: 6.947877  [107264/134204]\n",
      "loss: 8.324762  [120672/134204]\n",
      "loss: 7.188238  [134080/134204]\n",
      "Test Error: Avg loss: 9.658746 \n",
      "\n",
      "--- 45.49734711647034 seconds ---\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 6.857662  [    0/134204]\n",
      "loss: 5.428185  [13408/134204]\n",
      "loss: 5.406649  [26816/134204]\n",
      "loss: 7.003675  [40224/134204]\n",
      "loss: 5.897933  [53632/134204]\n",
      "loss: 8.194082  [67040/134204]\n",
      "loss: 6.512335  [80448/134204]\n",
      "loss: 7.314070  [93856/134204]\n",
      "loss: 6.439887  [107264/134204]\n",
      "loss: 8.621175  [120672/134204]\n",
      "loss: 9.129502  [134080/134204]\n",
      "Test Error: Avg loss: 9.646378 \n",
      "\n",
      "--- 48.58879089355469 seconds ---\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 6.208118  [    0/134204]\n",
      "loss: 6.209094  [13408/134204]\n",
      "loss: 5.765455  [26816/134204]\n",
      "loss: 7.684935  [40224/134204]\n",
      "loss: 7.783291  [53632/134204]\n",
      "loss: 9.173392  [67040/134204]\n",
      "loss: 8.123302  [80448/134204]\n",
      "loss: 7.369099  [93856/134204]\n",
      "loss: 7.636426  [107264/134204]\n",
      "loss: 9.841173  [120672/134204]\n",
      "loss: 8.972116  [134080/134204]\n",
      "Test Error: Avg loss: 9.629703 \n",
      "\n",
      "--- 48.44642400741577 seconds ---\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 9.688381  [    0/134204]\n",
      "loss: 7.369462  [13408/134204]\n",
      "loss: 9.185780  [26816/134204]\n",
      "loss: 8.541663  [40224/134204]\n",
      "loss: 5.134746  [53632/134204]\n",
      "loss: 7.929260  [67040/134204]\n",
      "loss: 5.941833  [80448/134204]\n",
      "loss: 5.680175  [93856/134204]\n",
      "loss: 8.329895  [107264/134204]\n",
      "loss: 7.012572  [120672/134204]\n",
      "loss: 6.608962  [134080/134204]\n",
      "Test Error: Avg loss: 9.625411 \n",
      "\n",
      "--- 47.561912059783936 seconds ---\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 7.911113  [    0/134204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.025599  [13408/134204]\n",
      "loss: 7.944249  [26816/134204]\n",
      "loss: 7.815312  [40224/134204]\n",
      "loss: 11.386112  [53632/134204]\n",
      "loss: 5.938230  [67040/134204]\n",
      "loss: 6.123808  [80448/134204]\n",
      "loss: 6.197783  [93856/134204]\n",
      "loss: 8.500117  [107264/134204]\n",
      "loss: 7.880980  [120672/134204]\n",
      "loss: 5.302892  [134080/134204]\n",
      "Test Error: Avg loss: 9.620515 \n",
      "\n",
      "--- 40.95484924316406 seconds ---\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 8.118155  [    0/134204]\n",
      "loss: 6.252625  [13408/134204]\n",
      "loss: 6.635734  [26816/134204]\n",
      "loss: 8.135188  [40224/134204]\n",
      "loss: 5.733853  [53632/134204]\n",
      "loss: 6.949522  [67040/134204]\n",
      "loss: 7.934990  [80448/134204]\n",
      "loss: 5.820335  [93856/134204]\n",
      "loss: 8.556893  [107264/134204]\n",
      "loss: 8.924479  [120672/134204]\n",
      "loss: 5.680575  [134080/134204]\n",
      "Test Error: Avg loss: 9.601979 \n",
      "\n",
      "--- 41.17728590965271 seconds ---\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 6.316889  [    0/134204]\n",
      "loss: 6.643032  [13408/134204]\n",
      "loss: 8.816549  [26816/134204]\n",
      "loss: 7.933661  [40224/134204]\n",
      "loss: 9.469090  [53632/134204]\n",
      "loss: 7.847218  [67040/134204]\n",
      "loss: 9.035501  [80448/134204]\n",
      "loss: 5.231204  [93856/134204]\n",
      "loss: 8.499286  [107264/134204]\n",
      "loss: 7.983912  [120672/134204]\n",
      "loss: 6.774720  [134080/134204]\n",
      "Test Error: Avg loss: 9.594464 \n",
      "\n",
      "--- 36.94815397262573 seconds ---\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 6.632465  [    0/134204]\n",
      "loss: 7.454746  [13408/134204]\n",
      "loss: 7.079605  [26816/134204]\n",
      "loss: 6.524655  [40224/134204]\n",
      "loss: 8.268312  [53632/134204]\n",
      "loss: 7.074281  [67040/134204]\n",
      "loss: 7.119542  [80448/134204]\n",
      "loss: 6.105103  [93856/134204]\n",
      "loss: 4.919556  [107264/134204]\n",
      "loss: 6.333770  [120672/134204]\n",
      "loss: 6.704960  [134080/134204]\n",
      "Test Error: Avg loss: 9.580202 \n",
      "\n",
      "--- 39.94496297836304 seconds ---\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 6.570896  [    0/134204]\n",
      "loss: 7.565566  [13408/134204]\n",
      "loss: 6.309035  [26816/134204]\n",
      "loss: 7.094225  [40224/134204]\n",
      "loss: 7.643379  [53632/134204]\n",
      "loss: 7.616419  [67040/134204]\n",
      "loss: 10.265855  [80448/134204]\n",
      "loss: 7.615227  [93856/134204]\n",
      "loss: 5.652861  [107264/134204]\n",
      "loss: 6.463131  [120672/134204]\n",
      "loss: 8.636971  [134080/134204]\n",
      "Test Error: Avg loss: 9.576868 \n",
      "\n",
      "--- 44.510268211364746 seconds ---\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 6.542755  [    0/134204]\n",
      "loss: 7.996197  [13408/134204]\n",
      "loss: 7.080263  [26816/134204]\n",
      "loss: 6.534177  [40224/134204]\n",
      "loss: 7.635636  [53632/134204]\n",
      "loss: 6.125208  [67040/134204]\n",
      "loss: 8.903864  [80448/134204]\n",
      "loss: 5.781163  [93856/134204]\n",
      "loss: 9.578095  [107264/134204]\n",
      "loss: 6.789154  [120672/134204]\n",
      "loss: 8.783598  [134080/134204]\n",
      "Test Error: Avg loss: 9.565062 \n",
      "\n",
      "--- 39.39414072036743 seconds ---\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 6.810101  [    0/134204]\n",
      "loss: 7.604203  [13408/134204]\n",
      "loss: 6.532090  [26816/134204]\n",
      "loss: 7.633316  [40224/134204]\n",
      "loss: 6.353944  [53632/134204]\n",
      "loss: 7.797172  [67040/134204]\n",
      "loss: 6.152037  [80448/134204]\n",
      "loss: 6.413980  [93856/134204]\n",
      "loss: 4.845180  [107264/134204]\n",
      "loss: 8.162310  [120672/134204]\n",
      "loss: 7.118013  [134080/134204]\n",
      "Test Error: Avg loss: 9.552024 \n",
      "\n",
      "--- 38.43558311462402 seconds ---\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 6.319015  [    0/134204]\n",
      "loss: 6.131557  [13408/134204]\n",
      "loss: 7.187565  [26816/134204]\n",
      "loss: 7.573215  [40224/134204]\n",
      "loss: 6.514778  [53632/134204]\n",
      "loss: 8.423380  [67040/134204]\n",
      "loss: 6.279574  [80448/134204]\n",
      "loss: 7.743124  [93856/134204]\n",
      "loss: 7.336690  [107264/134204]\n",
      "loss: 8.582670  [120672/134204]\n",
      "loss: 7.101098  [134080/134204]\n",
      "Test Error: Avg loss: 9.548971 \n",
      "\n",
      "--- 40.37503123283386 seconds ---\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 6.543156  [    0/134204]\n",
      "loss: 8.308163  [13408/134204]\n",
      "loss: 7.197656  [26816/134204]\n",
      "loss: 6.539848  [40224/134204]\n",
      "loss: 5.117919  [53632/134204]\n",
      "loss: 7.606747  [67040/134204]\n",
      "loss: 6.176820  [80448/134204]\n",
      "loss: 8.137863  [93856/134204]\n",
      "loss: 6.893141  [107264/134204]\n",
      "loss: 7.575332  [120672/134204]\n",
      "loss: 6.540029  [134080/134204]\n",
      "Test Error: Avg loss: 9.540919 \n",
      "\n",
      "--- 39.96808695793152 seconds ---\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 6.291777  [    0/134204]\n",
      "loss: 8.426213  [13408/134204]\n",
      "loss: 9.644956  [26816/134204]\n",
      "loss: 7.682702  [40224/134204]\n",
      "loss: 5.112568  [53632/134204]\n",
      "loss: 5.480239  [67040/134204]\n",
      "loss: 7.493860  [80448/134204]\n",
      "loss: 6.597863  [93856/134204]\n",
      "loss: 9.066366  [107264/134204]\n",
      "loss: 5.433892  [120672/134204]\n",
      "loss: 6.762532  [134080/134204]\n",
      "Test Error: Avg loss: 9.520109 \n",
      "\n",
      "--- 44.8255660533905 seconds ---\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 6.957842  [    0/134204]\n",
      "loss: 6.660819  [13408/134204]\n",
      "loss: 6.755803  [26816/134204]\n",
      "loss: 7.684249  [40224/134204]\n",
      "loss: 5.723436  [53632/134204]\n",
      "loss: 7.470360  [67040/134204]\n",
      "loss: 7.325064  [80448/134204]\n",
      "loss: 5.245643  [93856/134204]\n",
      "loss: 6.196886  [107264/134204]\n",
      "loss: 9.469011  [120672/134204]\n",
      "loss: 5.706995  [134080/134204]\n",
      "Test Error: Avg loss: 9.508401 \n",
      "\n",
      "--- 41.67032790184021 seconds ---\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 9.071621  [    0/134204]\n",
      "loss: 4.914634  [13408/134204]\n",
      "loss: 5.573555  [26816/134204]\n",
      "loss: 6.186788  [40224/134204]\n",
      "loss: 6.250067  [53632/134204]\n",
      "loss: 7.681225  [67040/134204]\n",
      "loss: 6.675231  [80448/134204]\n",
      "loss: 6.995227  [93856/134204]\n",
      "loss: 5.758103  [107264/134204]\n",
      "loss: 8.600792  [120672/134204]\n",
      "loss: 6.221485  [134080/134204]\n",
      "Test Error: Avg loss: 9.503679 \n",
      "\n",
      "--- 40.21422290802002 seconds ---\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 8.030025  [    0/134204]\n",
      "loss: 6.180031  [13408/134204]\n",
      "loss: 7.037546  [26816/134204]\n",
      "loss: 8.926633  [40224/134204]\n",
      "loss: 7.299924  [53632/134204]\n",
      "loss: 7.123766  [67040/134204]\n",
      "loss: 5.663184  [80448/134204]\n",
      "loss: 9.290345  [93856/134204]\n",
      "loss: 8.466797  [107264/134204]\n",
      "loss: 4.673111  [120672/134204]\n",
      "loss: 7.565332  [134080/134204]\n",
      "Test Error: Avg loss: 9.497021 \n",
      "\n",
      "--- 42.01343083381653 seconds ---\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 4.915990  [    0/134204]\n",
      "loss: 5.735655  [13408/134204]\n",
      "loss: 6.974213  [26816/134204]\n",
      "loss: 6.354859  [40224/134204]\n",
      "loss: 7.098108  [53632/134204]\n",
      "loss: 5.408445  [67040/134204]\n",
      "loss: 6.611543  [80448/134204]\n",
      "loss: 8.802102  [93856/134204]\n",
      "loss: 7.713646  [107264/134204]\n",
      "loss: 6.524464  [120672/134204]\n",
      "loss: 8.595076  [134080/134204]\n",
      "Test Error: Avg loss: 9.481087 \n",
      "\n",
      "--- 43.1243417263031 seconds ---\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 6.208759  [    0/134204]\n",
      "loss: 7.130623  [13408/134204]\n",
      "loss: 10.637109  [26816/134204]\n",
      "loss: 6.012274  [40224/134204]\n",
      "loss: 8.282307  [53632/134204]\n",
      "loss: 4.914130  [67040/134204]\n",
      "loss: 7.504518  [80448/134204]\n",
      "loss: 7.249149  [93856/134204]\n",
      "loss: 6.395769  [107264/134204]\n",
      "loss: 5.167538  [120672/134204]\n",
      "loss: 7.648993  [134080/134204]\n",
      "Test Error: Avg loss: 9.477192 \n",
      "\n",
      "--- 39.80350971221924 seconds ---\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 7.976596  [    0/134204]\n",
      "loss: 8.080281  [13408/134204]\n",
      "loss: 7.236624  [26816/134204]\n",
      "loss: 5.734757  [40224/134204]\n",
      "loss: 7.189992  [53632/134204]\n",
      "loss: 4.504781  [67040/134204]\n",
      "loss: 5.726290  [80448/134204]\n",
      "loss: 5.458874  [93856/134204]\n",
      "loss: 6.043821  [107264/134204]\n",
      "loss: 6.460771  [120672/134204]\n",
      "loss: 7.638220  [134080/134204]\n",
      "Test Error: Avg loss: 9.479077 \n",
      "\n",
      "--- 42.50615906715393 seconds ---\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 7.009216  [    0/134204]\n",
      "loss: 6.001283  [13408/134204]\n",
      "loss: 5.252365  [26816/134204]\n",
      "loss: 6.641930  [40224/134204]\n",
      "loss: 6.784465  [53632/134204]\n",
      "loss: 6.874188  [67040/134204]\n",
      "loss: 5.688873  [80448/134204]\n",
      "loss: 6.528167  [93856/134204]\n",
      "loss: 6.152691  [107264/134204]\n",
      "loss: 6.978227  [120672/134204]\n",
      "loss: 6.872133  [134080/134204]\n",
      "Test Error: Avg loss: 9.454375 \n",
      "\n",
      "--- 40.97928285598755 seconds ---\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 7.097146  [    0/134204]\n",
      "loss: 5.668319  [13408/134204]\n",
      "loss: 7.558029  [26816/134204]\n",
      "loss: 5.809180  [40224/134204]\n",
      "loss: 6.403522  [53632/134204]\n",
      "loss: 7.292611  [67040/134204]\n",
      "loss: 5.021546  [80448/134204]\n",
      "loss: 7.055843  [93856/134204]\n",
      "loss: 6.960246  [107264/134204]\n",
      "loss: 6.203337  [120672/134204]\n",
      "loss: 9.674128  [134080/134204]\n",
      "Test Error: Avg loss: 9.445468 \n",
      "\n",
      "--- 47.45194101333618 seconds ---\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 5.961426  [    0/134204]\n",
      "loss: 7.571834  [13408/134204]\n",
      "loss: 6.665464  [26816/134204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.519642  [40224/134204]\n",
      "loss: 7.748462  [53632/134204]\n",
      "loss: 7.353492  [67040/134204]\n",
      "loss: 5.998274  [80448/134204]\n",
      "loss: 6.432070  [93856/134204]\n",
      "loss: 6.086434  [107264/134204]\n",
      "loss: 4.496869  [120672/134204]\n",
      "loss: 6.316138  [134080/134204]\n",
      "Test Error: Avg loss: 9.442682 \n",
      "\n",
      "--- 47.225345849990845 seconds ---\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 6.127562  [    0/134204]\n",
      "loss: 5.867880  [13408/134204]\n",
      "loss: 7.896032  [26816/134204]\n",
      "loss: 6.944894  [40224/134204]\n",
      "loss: 6.631203  [53632/134204]\n",
      "loss: 6.159687  [67040/134204]\n",
      "loss: 7.534975  [80448/134204]\n",
      "loss: 9.146946  [93856/134204]\n",
      "loss: 8.581748  [107264/134204]\n",
      "loss: 7.002752  [120672/134204]\n",
      "loss: 6.261043  [134080/134204]\n",
      "Test Error: Avg loss: 9.435175 \n",
      "\n",
      "--- 40.49051284790039 seconds ---\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 5.941663  [    0/134204]\n",
      "loss: 9.264445  [13408/134204]\n",
      "loss: 6.330143  [26816/134204]\n",
      "loss: 5.860809  [40224/134204]\n",
      "loss: 7.916370  [53632/134204]\n",
      "loss: 6.538787  [67040/134204]\n",
      "loss: 5.934222  [80448/134204]\n",
      "loss: 8.467013  [93856/134204]\n",
      "loss: 6.586093  [107264/134204]\n",
      "loss: 6.258304  [120672/134204]\n",
      "loss: 7.029493  [134080/134204]\n",
      "Test Error: Avg loss: 9.424155 \n",
      "\n",
      "--- 46.34734892845154 seconds ---\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 8.014277  [    0/134204]\n",
      "loss: 5.042306  [13408/134204]\n",
      "loss: 8.530169  [26816/134204]\n",
      "loss: 6.015216  [40224/134204]\n",
      "loss: 5.994432  [53632/134204]\n",
      "loss: 7.468524  [67040/134204]\n",
      "loss: 5.701470  [80448/134204]\n",
      "loss: 7.909705  [93856/134204]\n",
      "loss: 7.346046  [107264/134204]\n",
      "loss: 6.095835  [120672/134204]\n",
      "loss: 6.448320  [134080/134204]\n",
      "Test Error: Avg loss: 9.427468 \n",
      "\n",
      "--- 45.818570375442505 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 60\n",
    "end_epoch = 100\n",
    "\n",
    "for epoch_n in range(start_epoch, end_epoch):\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch {epoch_n+1}\\n-------------------------------\")\n",
    "    train_loss = ps_train_loop(ps_train_dl, ps_model, p_s_loss, optimizer, device)\n",
    "    val_loss = ps_test_loop(ps_val_dl, ps_model, p_s_loss, device)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    ps_loss_d['epoch'].append(epoch_n)\n",
    "    ps_loss_d['train'].append(train_loss)\n",
    "    ps_loss_d['val'].append(val_loss)\n",
    "    if epoch_n % 3 == 0:\n",
    "        torch.save(ps_model.state_dict(), ps_save_path)\n",
    "        pd.DataFrame(ps_loss_d).to_csv(ps_loss_save_path, index=False)\n",
    "torch.save(ps_model.state_dict(), ps_save_path)\n",
    "pd.DataFrame(ps_loss_d).to_csv(ps_loss_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_ps_model = P_S_Network(len(user_d)+1, len(item_d)+1, 299)\n",
    "loaded_ps_model.load_state_dict(torch.load(ps_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A FAZER\n",
    "\n",
    "Pegar o dataset MIND SMALL\n",
    "\n",
    "- Checar se as listas de impressao sao geralmente escolhidas do inicio\n",
    "- Modificar modelo P_S.forward para receber beta como entrada. Ou criar um outro forward que pode ser usado com beta.\n",
    "- Documentar rapidamente as entradas e saidas. E checar a classe Dataset.\n",
    "- Implementar P_R\n",
    "    - Ter um método que recebe tau, para a política gaussiana\n",
    "- Treinar\n",
    "\n",
    "\n",
    "Reduzir o tamanho das imp_logs, opções:\n",
    "- Retirar todas que maiores que 5.\n",
    "- Truncar as listas em tamanho 5, retirando as que não possuem S nos 5 primeiros\n",
    "- Pegar os itens escolhidos e selecionar aleatoriamente mais outros para inteirar 5. Aleatorizar a ordem.\n",
    "- Dividir a lista em menores, o que pode significar reutilizar o S várias vezes.\n",
    "\n",
    "The negative samples are selected from the whole item set or the impression list. When optimizing 𝑝𝑹 and 𝑝𝑺 , we empirically set the learning rate as 0.001, and the user/item embedding sizes are both tunned in {16, 32, 64, 128, 256, 512}. The length of the impression list (i.e., |𝑹|) is set as 5, and the size of 𝑺 (i.e., 𝑘) is determined in {1, 2, 3}. The Gaussian policy is implemented as a two-layers fully- connected neural network, where the hidden dimension is searched in {16, 32, 64}.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
